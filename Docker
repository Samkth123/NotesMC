
Docker allows developers to deploy their applications in a sandbox (called contrainers) to run on the host operating system ie Linux.

The key benefit of Docker is that it allwos users to **package an application with all of its dependencies into a standardized unit**. Unlike Virtual Machines, containers do not have high overhead and hence enable for more efficient usage of the underlying system and resources.


### What are containers?

Today **industry standard is to run Virtual Machines** (VMs) to run software applications. VMs run on guest operating systems which run on virtual hardware powered by the servers host OS.

**VMs are great at providing full process isolation for applications** - there are few ways a problem in the host operating system can affect software running on the guest operating system (& vice versa). But this isolation has a great cost - the computational overhead spent virtualizing hardware for a guest OS can be substantial.

**Containers** take a differnet approach: leverage low level mechanics of the host operating sytems to provide isolation of virtual machines at a fraction of the computing power


### Why use containers?

Containers offer a packaging mechanism  that allows applications to be abstracted from the environment in which they actually run. This decoupling allows container based applications to be deployed easily and concistently regardless where it is ( the target environment could be anywhere, from public cloud to private data center to your own laptop). This makes it so that developers have the ability to create predicatable environments that are isolated from the rest of the applications and can basically run anywhere.



### Terminology

A Docker container is a runtime instance of a Docker image. It includes everything specified in the image plus a writable layer on top.

**Images**: The blueprints of our application which form the basis of containers. "docker pull" to download an image.

For simplicity, you can think of an image akin to a git repository - images can be commited with changes and have multiple versions. If you don't provide a specific version number, the client defaults to `latest`. For example, you can pull a specific version of `ubuntu` image
Containers: Create from Docker images and run the actual application. We create a container using "docker run". To see a list of running containers you run the "docker ps" command
- **Base images** are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.
    
- **Child images** are images that build on base images and add additional functionality.

**Docker Daemon**:  The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.

**Docker Client**: The command line tool that allows the user to interact with the daemon More generally there are other forms of clients

**Docker Hub**: A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.


Dockerfile: A simple text file that contains a list of commands that the Docker client calls while creating an image. Its a simple way of automating the image creation process. Dockerfile commands are almost identical to Linux commands.

### MULTI-CONTAINER ENVIRONMENTS

Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.


Just like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the **services** separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.

#### Docker Network

The **bridge** network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.

docker network inspect bridge

You can see that our container `277451c15ec1` is listed under the `Containers` section in the output. What we also see is the IP address this container has been allotted - `172.17.0.2`. Is this the IP address that we're looking for?

Since the _bridge_ network is shared by every container by default, this method is **not secure**. How do we isolate our network?

The good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the `docker network` command.


The `network create` command creates a new _bridge_ network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network

Now that we have a network, we can launch our containers inside this network using the `--net` flag

#### Docker Compose

A tool for defining and running multi-container Docker applications.

So what is _Compose_ used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called `docker-compose.yml` that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.







